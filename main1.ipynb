{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "import zipfile\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime # Ensure datetime is imported if not already at the top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating clients\n",
    "aws_lambda = boto3.client('lambda', region_name='us-east-1')\n",
    "iam_client = boto3.client('iam')\n",
    "s3_client = boto3.client('s3', region_name='us-east-1')\n",
    "sqs = boto3.client('sqs', region_name=\"us-east-1\")\n",
    "dynamodb = boto3.resource('dynamodb', region_name='us-east-1')\n",
    "dynamodb_client = boto3.client('dynamodb', region_name='us-east-1')\n",
    "role = iam_client.get_role(RoleName='LabRole')\n",
    "role_arn = role['Role']['Arn']\n",
    "sns_client = boto3.client(\"sns\", region_name='us-east-1')\n",
    "lambda_client = boto3.client('lambda', region_name= 'us-east-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket final-summer99d created successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Create S3 Bucket (Incoming Data Bucket) ---\n",
    "def create_bucket(bucket_name, region='us-east-1'):\n",
    "    \"\"\"Create an S3 bucket in a specified region.\"\"\"\n",
    "    try:\n",
    "        if region == 'us-east-1':\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            location = {'LocationConstraint': region}\n",
    "            s3_client.create_bucket(Bucket=bucket_name, CreateBucketConfiguration=location)\n",
    "        print(f'Bucket {bucket_name} created successfully.')\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'BucketAlreadyOwnedByYou':\n",
    "            print(f'Bucket {bucket_name} already exists and is owned by you.')\n",
    "        elif e.response['Error']['Code'] == 'BucketAlreadyExists':\n",
    "            print(f'Bucket {bucket_name} already exists (owned by another account or globally).')\n",
    "        else:\n",
    "            logging.error(e)\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "incoming_bucket_name = \"final-summer99d\"\n",
    "create_bucket(incoming_bucket_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Uploading user_email.json to S3 bucket 'final-summer99d' ---\n",
      "Successfully uploaded user_email.json to s3://final-summer99d/config/user_email.json.\n"
     ]
    }
   ],
   "source": [
    "# --- Upload User Emails Configuration to S3 ---\n",
    "import os # Ensure os is imported\n",
    "\n",
    "user_emails_file_path = 'user_email.json' # Make sure this file exists in your project root\n",
    "s3_config_key = 'config/user_email.json' # S3 path where the config file will be stored\n",
    "\n",
    "if incoming_bucket_name and os.path.exists(user_emails_file_path):\n",
    "    print(f\"\\n--- Uploading {user_emails_file_path} to S3 bucket '{incoming_bucket_name}' ---\")\n",
    "    try:\n",
    "        s3_client.upload_file(\n",
    "            Filename=user_emails_file_path,\n",
    "            Bucket=incoming_bucket_name,\n",
    "            Key=s3_config_key\n",
    "        )\n",
    "        print(f\"Successfully uploaded {user_emails_file_path} to s3://{incoming_bucket_name}/{s3_config_key}.\")\n",
    "    except ClientError as e:\n",
    "        print(f\"Error uploading user emails config to S3: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error uploading user emails config to S3: {str(e)}\")\n",
    "else:\n",
    "    print(f\"Skipping user emails config upload: '{user_emails_file_path}' not found or S3 bucket not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynamoDB table CyclicalBetaSurvey already exists.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----- Create a DynamoDB table ----\n",
    "def create_dynamodb_table(table_name):\n",
    "    \"\"\"Create a DynamoDB table for survey responses.\"\"\"\n",
    "    try:\n",
    "        dynamodb_client.create_table(\n",
    "            TableName=table_name,\n",
    "            KeySchema=[\n",
    "                {'AttributeName': 'user_id', 'KeyType': 'HASH'},  # Partition key\n",
    "                {'AttributeName': 'timestamp', 'KeyType': 'RANGE'}  # Sort key\n",
    "            ],\n",
    "            AttributeDefinitions=[\n",
    "                {'AttributeName': 'user_id', 'AttributeType': 'S'},\n",
    "                {'AttributeName': 'timestamp', 'AttributeType': 'S'}\n",
    "            ],\n",
    "            ProvisionedThroughput={\n",
    "                'ReadCapacityUnits': 5,\n",
    "                'WriteCapacityUnits': 5\n",
    "            }\n",
    "        )\n",
    "        dynamodb_client.get_waiter('table_exists').wait(TableName=table_name)\n",
    "        print(f'DynamoDB table {table_name} created successfully.')\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'ResourceInUseException':\n",
    "            print(f'DynamoDB table {table_name} already exists.')\n",
    "        else:\n",
    "            logging.error(e)\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Setting beta table name\n",
    "table_name = 'CyclicalBetaSurvey'\n",
    "\n",
    "# Call the function to create the table\n",
    "create_dynamodb_table(table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name= \"final-summer99d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper for Zipping Lambda Code ---\n",
    "def create_lambda_zip(file_name, zip_name):\n",
    "    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        zf.write(file_name, arcname=os.path.basename(file_name))\n",
    "    print(f\"{zip_name} created from {file_name}.\")\n",
    "\n",
    "# --- Helper for creating/updating Lambda Function ---\n",
    "def deploy_lambda_function(function_name, handler, runtime, role_arn, zip_file_path, timeout=30):\n",
    "    create_lambda_zip(handler.split('.')[0] + '.py', zip_file_path)\n",
    "    with open(zip_file_path, 'rb') as f:\n",
    "        lambda_zip_content = f.read()\n",
    "\n",
    "    try:\n",
    "        aws_lambda.create_function(\n",
    "            FunctionName=function_name,\n",
    "            Runtime=runtime,\n",
    "            Role=role_arn,\n",
    "            Handler=handler,\n",
    "            Code=dict(ZipFile=lambda_zip_content),\n",
    "            Timeout=timeout\n",
    "        )\n",
    "        print(f\"{function_name} function created.\")\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'ResourceConflictException':\n",
    "            print(f\"{function_name} already exists. Attempting to update code.\")\n",
    "            aws_lambda.update_function_code(\n",
    "                FunctionName=function_name,\n",
    "                ZipFile=lambda_zip_content\n",
    "            )\n",
    "            aws_lambda.update_function_configuration(\n",
    "                FunctionName=function_name,\n",
    "                Handler=handler, # Update handler in case it changed\n",
    "                Runtime=runtime, # Update runtime in case it changed\n",
    "                Role=role_arn,   # Update role in case it changed\n",
    "                Timeout=timeout\n",
    "            )\n",
    "            print(f\"{function_name} function code and configuration updated.\")\n",
    "        else:\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verify.zip created from verify.py.\n",
      "verify function created.\n",
      "cat_lambda.zip created from cat_lambda.py.\n",
      "cat_lambda function created.\n",
      "rec_lambda.zip created from rec_lambda.py.\n",
      "rec_lambda function created.\n",
      "send_email_lambda.zip created from send_email_lambda.py.\n",
      "send_email_lambda function created.\n"
     ]
    }
   ],
   "source": [
    "# --- Deploy Lambda Functions ---\n",
    "if role_arn:\n",
    "    # verify (initial validation) Lambda function\n",
    "    # The name of the Lambda function in AWS will be 'verify'.\n",
    "    # This deploys the 'verify.py' file.\n",
    "    lambda_name_verify = 'verify'  # Correctly define the function name\n",
    "    deploy_lambda_function(\n",
    "    function_name=lambda_name_verify,  # Use the defined variable\n",
    "    handler='verify.lambda_handler',   # Points to verify.py and lambda_handler\n",
    "    runtime='python3.9',\n",
    "    role_arn=role_arn,\n",
    "    zip_file_path=f'{lambda_name_verify}.zip'  # This will look for 'verify.zip'\n",
    ")\n",
    "\n",
    "    # cat_lambda\n",
    "    lambda_name_cat = 'cat_lambda'\n",
    "    deploy_lambda_function(\n",
    "        function_name=lambda_name_cat,\n",
    "        handler='cat_lambda.lambda_handler',\n",
    "        runtime='python3.9',\n",
    "        role_arn=role_arn,\n",
    "        zip_file_path=f'{lambda_name_cat}.zip'\n",
    "    )\n",
    "\n",
    "    # rec_lambda\n",
    "    lambda_name_rec = 'rec_lambda'\n",
    "    deploy_lambda_function(\n",
    "        function_name=lambda_name_rec,\n",
    "        handler='rec_lambda.lambda_handler',\n",
    "        runtime='python3.9',\n",
    "        role_arn=role_arn,\n",
    "        zip_file_path=f'{lambda_name_rec}.zip'\n",
    "    )\n",
    "\n",
    "    lambda_name_send_email = 'send_email_lambda'  # Correctly define the function name\n",
    "    deploy_lambda_function(\n",
    "    function_name=lambda_name_send_email,  # Use the defined variable\n",
    "    handler='send_email_lambda.lambda_handler',   # Points to verify.py and lambda_handler\n",
    "    runtime='python3.9',\n",
    "    role_arn=role_arn,\n",
    "    zip_file_path=f'{lambda_name_send_email}.zip'  # This will look for 'verify.zip'\n",
    ")\n",
    "else:\n",
    "    print(\"Skipping Lambda deployments due to missing IAM Role.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'd0d97c93-c2af-4126-a326-f7dd9464c5ba',\n",
       "  'HTTPStatusCode': 201,\n",
       "  'HTTPHeaders': {'date': 'Fri, 30 May 2025 20:12:23 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '298',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'd0d97c93-c2af-4126-a326-f7dd9464c5ba'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Statement': '{\"Sid\":\"s3-invoke\",\"Effect\":\"Allow\",\"Principal\":{\"Service\":\"s3.amazonaws.com\"},\"Action\":\"lambda:InvokeFunction\",\"Resource\":\"arn:aws:lambda:us-east-1:544835564974:function:verify\",\"Condition\":{\"ArnLike\":{\"AWS:SourceArn\":\"arn:aws:s3:::final-summer99d\"}}}'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add permission for S3 to invoke the Lambda function\n",
    "lambda_client.add_permission(\n",
    "    FunctionName='verify',  # The name of your Lambda function\n",
    "    StatementId='s3-invoke',  # A unique identifier for this permission\n",
    "    Action='lambda:InvokeFunction',  # The action S3 is allowed to perform\n",
    "    Principal='s3.amazonaws.com',  # The service allowed to invoke the function\n",
    "    SourceArn=f'arn:aws:s3:::{bucket_name}'  # The ARN of your S3 bucket\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up S3 event notification for bucket final-summer99d to trigger Lambda 'verify'.\n"
     ]
    }
   ],
   "source": [
    "# Set up S3 event notification to trigger Lambda\n",
    "try:\n",
    "    s3_client.put_bucket_notification_configuration(\n",
    "        Bucket=bucket_name,\n",
    "        NotificationConfiguration={\n",
    "            'LambdaFunctionConfigurations': [\n",
    "                {\n",
    "                    'LambdaFunctionArn': aws_lambda.get_function(FunctionName='verify')['Configuration']['FunctionArn'],\n",
    "                    'Events': ['s3:ObjectCreated:*'],\n",
    "                    'Filter': {\n",
    "                        'Key': {\n",
    "                            'FilterRules': [\n",
    "                                {'Name': 'prefix', 'Value': 'raw_surveys/'}\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    print(f\"Set up S3 event notification for bucket {bucket_name} to trigger Lambda 'verify'.\")\n",
    "except ClientError as e:\n",
    "    print(f\"Error setting up S3 event notification: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQS queue created: https://sqs.us-east-1.amazonaws.com/544835564974/BetaSurveyQueuesummer99d\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create SQS Queue\n",
    "queue_url = sqs.create_queue(QueueName='BetaSurveyQueuesummer99d')['QueueUrl']\n",
    "print(f\"SQS queue created: {queue_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQS ARN: arn:aws:sqs:us-east-1:544835564974:BetaSurveyQueuesummer99d\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Get SQS Queue ARN\n",
    "sqs_info = sqs.get_queue_attributes(QueueUrl=queue_url, AttributeNames=['QueueArn'])\n",
    "sqs_arn = sqs_info['Attributes']['QueueArn']\n",
    "print(f\"SQS ARN: {sqs_arn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue_name_initial=\"BetaSurveyQueuesummer99d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQS queue linked to Lambda function.\n",
      "SQS Queue URL: https://sqs.us-east-1.amazonaws.com/544835564974/BetaSurveyQueuesummer99d\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Connect SQS to Lambda\n",
    "aws_lambda.create_event_source_mapping(\n",
    "    EventSourceArn=sqs_arn,\n",
    "    FunctionName='verify',\n",
    "    Enabled=True,\n",
    "    BatchSize=10\n",
    ")\n",
    "print(\"SQS queue linked to Lambda function.\")\n",
    "\n",
    "print(f\"SQS Queue URL: {queue_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Uploading survey files to S3 bucket 'final-summer99d' (this will trigger verify Lambda) --- \n",
      "Uploaded raw_surveys/test1_20250530151800328749.json to S3 bucket final-summer99d with status: 200\n",
      "Upload of tests/test1.json to S3 completed with status: 200\n",
      "Uploaded raw_surveys/test2_20250530151802808462.json to S3 bucket final-summer99d with status: 200\n",
      "Upload of tests/test2.json to S3 completed with status: 200\n",
      "Uploaded raw_surveys/test3_20250530151805277418.json to S3 bucket final-summer99d with status: 200\n",
      "Upload of tests/test3.json to S3 completed with status: 200\n",
      "Uploaded raw_surveys/test4_20250530151807463619.json to S3 bucket final-summer99d with status: 200\n",
      "Upload of tests/test4.json to S3 completed with status: 200\n",
      "Uploaded raw_surveys/test5_20250530151809668412.json to S3 bucket final-summer99d with status: 200\n",
      "Upload of tests/test5.json to S3 completed with status: 200\n",
      "Uploaded raw_surveys/test6_20250530151811871184.json to S3 bucket final-summer99d with status: 200\n",
      "Upload of tests/test6.json to S3 completed with status: 200\n",
      "Uploaded raw_surveys/test7_20250530151814134858.json to S3 bucket final-summer99d with status: 200\n",
      "Upload of tests/test7.json to S3 completed with status: 200\n"
     ]
    }
   ],
   "source": [
    "# --- Simulate Survey Data Ingestion (Upload to S3) ---\n",
    "\n",
    "def upload_survey_to_s3(survey_path, bucket_name, prefix='raw_surveys/'):\n",
    "    s3 = boto3.client('s3', region_name='us-east-1')\n",
    "    \n",
    "    # Read the JSON survey file\n",
    "    with open(survey_path, 'r') as f:\n",
    "        survey_data = json.load(f) # survey_data is now either a dict or a list\n",
    "    \n",
    "    # Ensure each test.json is a list as expected by verify.py\n",
    "    if not isinstance(survey_data, list):\n",
    "        # If a single JSON object, wrap it in a list for verify.py to process\n",
    "        survey_data_to_upload = [survey_data] # Use a new variable name for clarity\n",
    "    else:\n",
    "        survey_data_to_upload = survey_data # If already a list, use it as is\n",
    "    \n",
    "    # Create a unique key for the S3 object\n",
    "    # Example: raw_surveys/test1_TIMESTAMP.json\n",
    "    timestamp_now = datetime.now().strftime(\"%Y%m%d%H%M%S%f\") # More unique timestamp\n",
    "    file_base_name = os.path.basename(survey_path).replace('.json', '')\n",
    "    s3_key = f\"{prefix}{file_base_name}_{timestamp_now}.json\"\n",
    "    \n",
    "    # Upload the survey data (as a JSON string) to S3\n",
    "    try:\n",
    "        s3.put_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=s3_key,\n",
    "            Body=json.dumps(survey_data_to_upload), # <-- CRITICAL: Use survey_data_to_upload here\n",
    "            ContentType='application/json'\n",
    "        )\n",
    "        print(f\"Uploaded {s3_key} to S3 bucket {bucket_name} with status: 200\")\n",
    "        return 200\n",
    "    except ClientError as e:\n",
    "        print(f\"Error uploading {s3_key} to S3: {str(e)}\")\n",
    "        return 400\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error uploading {s3_key} to S3: {str(e)}\")\n",
    "        return 500\n",
    "\n",
    "# List of survey files (assuming these are in a 'tests/' subdirectory)\n",
    "survey_files = [\n",
    "    'tests/test1.json',\n",
    "    'tests/test2.json',\n",
    "    'tests/test3.json',\n",
    "    'tests/test4.json',\n",
    "    'tests/test5.json',\n",
    "    'tests/test6.json',\n",
    "    'tests/test7.json'\n",
    "]\n",
    "\n",
    "# The bucket name is 'final-summer99d' as defined earlier in main1.ipynb\n",
    "if bucket_name: # Use the variable 'incoming_bucket_name'\n",
    "    print(f\"\\n--- Uploading survey files to S3 bucket '{bucket_name}' (this will trigger verify Lambda) --- \")\n",
    "    for survey_file in survey_files:\n",
    "        status = upload_survey_to_s3(survey_file, bucket_name)\n",
    "        print(f\"Upload of {survey_file} to S3 completed with status: {status}\")\n",
    "        time.sleep(2) # Give some time for Lambda to be triggered and process\n",
    "else:\n",
    "    print(\"Skipping survey uploads: Incoming S3 bucket name is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket contents:\n",
      "config/user_email.json\n",
      "raw_surveys/test1_20250530151234499348.json\n",
      "raw_surveys/test1_20250530151708027171.json\n",
      "raw_surveys/test1_20250530151800328749.json\n",
      "raw_surveys/test2_20250530151236687784.json\n",
      "raw_surveys/test2_20250530151710251814.json\n",
      "raw_surveys/test2_20250530151802808462.json\n",
      "raw_surveys/test3_20250530151238880982.json\n",
      "raw_surveys/test3_20250530151712450763.json\n",
      "raw_surveys/test3_20250530151805277418.json\n",
      "raw_surveys/test4_20250530151241123513.json\n",
      "raw_surveys/test4_20250530151714676573.json\n",
      "raw_surveys/test4_20250530151807463619.json\n",
      "raw_surveys/test5_20250530151243329801.json\n",
      "raw_surveys/test5_20250530151716879977.json\n",
      "raw_surveys/test5_20250530151809668412.json\n",
      "raw_surveys/test6_20250530151245565438.json\n",
      "raw_surveys/test6_20250530151719149441.json\n",
      "raw_surveys/test6_20250530151811871184.json\n",
      "raw_surveys/test7_20250530151247771739.json\n",
      "raw_surveys/test7_20250530151721343748.json\n",
      "raw_surveys/test7_20250530151814134858.json\n",
      "recommendations/user_001_052724100245.json\n",
      "recommendations/user_002_052724121105.json\n",
      "recommendations/user_003_052724234500.json\n",
      "recommendations/user_004_052724121105.json\n",
      "recommendations/user_005_052724143205.json\n"
     ]
    }
   ],
   "source": [
    "# Verify S3 bucket\n",
    "print(\"S3 bucket contents:\")\n",
    "response = s3_client.list_objects_v2(Bucket='final-summer99d')\n",
    "objects = response.get('Contents', [])\n",
    "if objects:\n",
    "    for obj in objects:\n",
    "        print(obj['Key'])\n",
    "else:\n",
    "    print(\"No objects found in S3 bucket final-summer99d.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verifying DynamoDB records... ---\n",
      "Found 5 records in DynamoDB table 'CyclicalBetaSurvey':\n",
      "{'recommendations': ['Get plenty of rest and iron-rich foods like spinach and lentils.', 'Do gentle yoga or stretching instead of high-intensity workouts.', 'Stay hydrated and use heat pads for cramps.'], 'user_id': 'user_001', 'phase': 'Menstruation', 'responses': {'q1': Decimal('4'), 'q2': Decimal('0'), 'q3': Decimal('1'), 'q4': Decimal('1'), 'q5': {'symptoms': ['Cramps', 'Feeling hot or flushed', 'Digestive issues (constipation, diarrhea)'], 'additional': 'Felt dizzy after waking up.'}, 'q6': Decimal('1')}, 'timestamp': '052724100245', 'valid': True, 'time_elapsed': Decimal('10')}\n",
      "{'recommendations': ['Get plenty of rest and iron-rich foods like spinach and lentils.', 'Do gentle yoga or stretching instead of high-intensity workouts.', 'Stay hydrated and use heat pads for cramps.'], 'user_id': 'user_005', 'phase': 'Menstruation', 'responses': {'q1': Decimal('3'), 'q2': Decimal('2'), 'q3': Decimal('4'), 'q4': Decimal('4'), 'q5': {'symptoms': ['Breast tenderness', 'Lower back pain'], 'additional': 'Mild fatigue but manageable.'}, 'q6': Decimal('4')}, 'timestamp': '052724143205', 'valid': True, 'time_elapsed': Decimal('11.1')}\n",
      "{'recommendations': ['Eat zinc-rich foods like pumpkin seeds for hormone support.', 'Socialize and schedule big meetings—confidence peaks now.', 'Engage in high-intensity or group workouts.'], 'user_id': 'user_003', 'phase': 'Ovulation', 'responses': {'q1': Decimal('0'), 'q2': Decimal('4'), 'q3': Decimal('5'), 'q4': Decimal('5'), 'q5': {'symptoms': ['Nothing noticeable today'], 'additional': ''}, 'q6': Decimal('5')}, 'timestamp': '052724234500', 'valid': True, 'time_elapsed': Decimal('8')}\n",
      "{'recommendations': ['Eat magnesium-rich foods (dark chocolate, leafy greens) to ease PMS.', 'Practice mindfulness or journaling to regulate mood.', 'Switch to moderate exercise like pilates or walking.'], 'user_id': 'user_002', 'phase': 'Luteal', 'responses': {'q1': Decimal('1'), 'q2': Decimal('3'), 'q3': Decimal('3'), 'q4': Decimal('2'), 'q5': {'symptoms': ['Headache or migraine', 'Acne or skin breakouts'], 'additional': 'Tension around temples.'}, 'q6': Decimal('2')}, 'timestamp': '052724121105', 'valid': True, 'time_elapsed': Decimal('13.2')}\n",
      "{'recommendations': ['Get plenty of rest and iron-rich foods like spinach and lentils.', 'Do gentle yoga or stretching instead of high-intensity workouts.', 'Stay hydrated and use heat pads for cramps.'], 'user_id': 'user_004', 'phase': 'Menstruation', 'responses': {'q1': Decimal('2'), 'q2': Decimal('1'), 'q3': Decimal('2'), 'q4': Decimal('3'), 'q5': {'symptoms': ['Cramps', 'Bloating', 'Breast fullness or swelling'], 'additional': 'Mild constipation today.'}, 'q6': Decimal('3')}, 'timestamp': '052724121105', 'valid': True, 'time_elapsed': Decimal('24.2')}\n"
     ]
    }
   ],
   "source": [
    "table_name='CyclicalBetaSurvey'\n",
    "# --- Verify DynamoDB Records ---\n",
    "print(\"\\n--- Verifying DynamoDB records... ---\")\n",
    "table = dynamodb.Table(table_name)\n",
    "try:\n",
    "    dynamodb_records = table.scan()['Items']\n",
    "    if dynamodb_records:\n",
    "        print(f\"Found {len(dynamodb_records)} records in DynamoDB table '{table_name}':\")\n",
    "        for record in dynamodb_records:\n",
    "            print(record)\n",
    "    else:\n",
    "        print(f\"No records found in DynamoDB table '{table_name}'.\")\n",
    "except ClientError as e:\n",
    "    print(f\"Error scanning DynamoDB table '{table_name}': {str(e)}\")\n",
    "    table_description = dynamodb_client.describe_table(TableName=table_name)\n",
    "    print(f\"Table status: {table_description['Table']['TableStatus']}\")\n",
    "    print(f\"Table item count: {table_description['Table']['ItemCount']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda function verify not found\n",
      "Lambda function cat_lambda not found\n",
      "Lambda function rec_lambda not found\n",
      "Deleted Lambda function: send_email_lambda\n",
      "SQS Queue Already Deleted\n",
      "DynamoDB Table Already Deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete each pipeline component if it still exists:\n",
    "\n",
    "# Lambdas\n",
    "lambda_functions=['verify', 'cat_lambda','rec_lambda', \"send_email_lambda\"]\n",
    "for function_name in lambda_functions:\n",
    "    try:\n",
    "        lambda_client.delete_function(FunctionName=function_name)\n",
    "        print(f\"Deleted Lambda function: {function_name}\")\n",
    "    except lambda_client.exceptions.ResourceNotFoundException:\n",
    "        print(f\"Lambda function {function_name} not found\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting Lambda function {function_name}: {e}\")\n",
    "\n",
    "# SQS\n",
    "try:\n",
    "    sqs.delete_queue(QueueUrl='https://sqs.us-east-1.amazonaws.com/544835564974/BetaSurveyQueuesummer99d')\n",
    "    print(\"SQS Queue Deleted\")\n",
    "except sqs.exceptions.QueueDoesNotExist:\n",
    "    print(\"SQS Queue Already Deleted\")\n",
    "\n",
    "# DynamoDB\n",
    "dynamodb = boto3.client('dynamodb', region_name='us-east-1')\n",
    "try:\n",
    "    response = dynamodb.delete_table(TableName='CyclicalBetaSurvey')\n",
    "    print(\"DynamoDB Table Deleted\")\n",
    "except dynamodb.exceptions.ResourceNotFoundException:\n",
    "    print(\"DynamoDB Table Already Deleted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
